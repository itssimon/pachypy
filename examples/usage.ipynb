{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to pachypy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T09:15:06.172676Z",
     "start_time": "2019-04-07T09:15:06.169902Z"
    }
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a `PrettyPachydermClient` instance, optimized for usage in a Jupyter Notebook. It inherits from [`PachydermClient`][1] and just adds some eye candy. If you're not in a Jupyter Notebook, you probably want to use [`PachydermClient`][1] directly.\n",
    "\n",
    "[1]: https://pachypy.readthedocs.io/en/latest/apidoc/pachypy.html#pachypy.client.PachydermClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T08:56:24.811848Z",
     "start_time": "2019-04-07T08:56:24.798492Z"
    }
   },
   "outputs": [],
   "source": [
    "import pachypy\n",
    "\n",
    "client = pachypy.PrettyPachydermClient(\n",
    "    # If not specified, host and port are inferred from the PACHD_ADDRESS\n",
    "    # environment variable (like pachctl), or default to localhost:30650.\n",
    "    host=None,  \n",
    "    port=None,\n",
    "    \n",
    "    # Add image digests when creating/updating pipelines.\n",
    "    # This forces new images to be used instead of cached ones,\n",
    "    # even if image name and tag remain the same.\n",
    "    add_image_digests=True,\n",
    "    \n",
    "    # Automatically build Docker images when creating/updating pipelines.\n",
    "    # Only applies for pipelines that have the transform.dockerfile or\n",
    "    # transform.dockerfile_path field set. Images will also be pushed to the registry.\n",
    "    build_images=True,\n",
    "    \n",
    "    # Specify one or multiple glob patterns to find your pipeline specification files.\n",
    "    # Pipeline specifications can be read from YAML and JSON files (see pipelines.yaml example).\n",
    "    pipeline_spec_files=['../pipelines/*.yaml', '../more_pipelines/*.json'],\n",
    "    \n",
    "    # You can specify a custom pipeline spec transformer function,\n",
    "    # which will be applied to every pipeline specification read.\n",
    "    # For example, you could define a function that adds secrets to all pipelines without\n",
    "    # having to specify them individually in every pipeline specification.\n",
    "    pipeline_spec_transformer=add_secrets_to_pipeline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pachypy uses a [`DockerClient`](https://docker-py.readthedocs.io/en/stable/client.html#docker.client.DockerClient) to build images, push them to registries and to retrieve image digests. By default, the `DockerClient` is initialized [from environment variables](https://docker-py.readthedocs.io/en/stable/client.html#docker.client.from_env), but you can always create your own instance and tell pachypy to use that one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T09:21:07.238718Z",
     "start_time": "2019-04-07T09:21:07.229836Z"
    }
   },
   "outputs": [],
   "source": [
    "import docker\n",
    "\n",
    "# This is not normally necessary if you have Docker running locally with default settings\n",
    "client.docker_client = docker.DockerClient(\n",
    "    base_url='tcp://12.34.56.78:1234',\n",
    "    tls=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Amazon ECR as a private container registry, pachypy has some functionality built in to simplify your workflow. It automatically retrieves an authorization token from AWS and logs your `DockerClient` in. It also retrieves image digests more efficiently by using the special AWS ECR API instead of the standard Docker API.\n",
    "\n",
    "This is done using the [`boto3`](https://github.com/boto/boto3) package. By default, it [reads your AWS credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#configuration) from `~/.aws/credentials` or environment variables. But you can also specify your AWS credentials manually, as shown below, although that is not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-07T09:35:01.854104Z",
     "start_time": "2019-04-07T09:35:01.073446Z"
    }
   },
   "outputs": [],
   "source": [
    "# You only have to do this if you have to manually specify AWS credentials\n",
    "client.amazon_ecr.set_credentials(\n",
    "    aws_access_key_id='<YOUR_KEY_ID>',\n",
    "    aws_secret_access_key='<YOUR_SECRET_ACCESS_KEY>',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When listing Pachyderm objects, pachypy allows you to use wildcard patterns to specify what you want to retrieve. Objects are matched against these patterns using [`fnmatch`](https://docs.python.org/3/library/fnmatch.html#fnmatch.fnmatch).\n",
    "\n",
    "The `PrettyPachydermClient` also outputs tables with nice formatting and shows progress bars for larger operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last 20 jobs for all pipelines starting with 'example'\n",
    "client.list_jobs('example*', n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pipelines starting with example*\n",
    "client.list_pipelines('example*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all repos starting with example*\n",
    "client.list_repos('example*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CSV files in the staging branch of repos starting with 'example'\n",
    "client.list_files('example*', branch='staging', glob='**.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the last 10 commits in each repo starting with 'example'\n",
    "client.list_commits('example*', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datums for the job with ID 'abcd1234' (no wildcards here)\n",
    "client.list_datums('abcd1234')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user logs from the last jobs of all pipelines starting with 'example'\n",
    "# in a nicely formatted way, grouped by pipeline and worker\n",
    "client.get_logs('example*', last_job_only=True, user_only=True, master=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list commands return an object that contains both the HTML output and the actual pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean duration of the last 100 jobs of pipeline 'example1'\n",
    "output = client.list_jobs('example1', n=100)\n",
    "output.df['Duration'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating objects\n",
    "\n",
    "Wildcards can also be used in many of the manipulation functions for batch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one or multiple Pachyderm repositories\n",
    "client.create_repos('example1')\n",
    "client.create_repos(['example2', 'example3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all repositories starting with 'example'\n",
    "client.delete_repos('example*')\n",
    "\n",
    "# Delete all repositories starting with 'example' or 'test'\n",
    "client.delete_repos(['example*', 'test*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark a commit as branch 'staging' in the 'example1' repository\n",
    "client.create_branch('example1', commit='abcd1234', branch='staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'staging' branch in the 'example1' repository\n",
    "# (while keeping the referenced commit intact)\n",
    "client.delete_branch('example', 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into creating pipelines, check out the [`pipelines.yaml`](https://github.com/itssimon/pachypy/blob/master/examples/pipelines.yaml) example file to see what pipeline specifications can look like when using pachypy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all pipelines starting with 'example'.\n",
    "# This will go through the pipeline specification files specified above\n",
    "# to find matching pipeline specs.\n",
    "client.create_pipelines('example*')\n",
    "\n",
    "# Like above, but deletes existing pipelines first before recreating them.\n",
    "client.create_pipelines('example*', recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all pipelines starting with 'example'\n",
    "client.update_pipelines('example*')\n",
    "\n",
    "# Like above, but also reprocesses all the data (see Pachyderm docs)\n",
    "client.update_pipelines('example*', reprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all pipelines starting with 'example'\n",
    "client.delete_pipelines('example*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start and stop all pipelines starting with 'example'\n",
    "client.start_pipelines('example*')\n",
    "client.stop_pipelines('example*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger a job for the pipeline 'example1' (no wildcards)\n",
    "# by committing a timestamp file into its cron input repository.\n",
    "client.trigger_pipeline('example1')\n",
    "\n",
    "# Same as above, but block until the job has finished and results are available\n",
    "client.trigger_pipeline('example1', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete job with ID 'abcd1234' (no wildcards)\n",
    "client.delete_job('abcd1234')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files in the master branch of the 'example1' repository to the current directory\n",
    "client.get_files('example1')\n",
    "\n",
    "# Download all CSV files under '/some/path' in the 'example1' repository (master branch)\n",
    "# to a local folder 'example_files'\n",
    "client.get_files('example1', path='/some/path', glob='*.csv', destination='./example_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the content of a single file as a string\n",
    "# while automatically detecting the character encoding\n",
    "client.get_file_content('example1', '/some/path/to/file.csv', encoding='auto')\n",
    "\n",
    "# Retrieves the content of a single file as a bytes object\n",
    "client.get_file_content('example1', '/some/path/to/image.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits\n",
    "\n",
    "pachypy uses a context manager for commits, so you don't have to worry about starting and finishing a commit manually. If an exception is raised and not caught during the commit it is automatically cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new commit in branch 'staging' of repo 'example1',\n",
    "# delete file 'some_file' and upload a local file with the same name.\n",
    "# The commit is automatically finished by the context manager.\n",
    "with client.commit('example1', branch='staging') as commit:\n",
    "    commit.delete_file('/some_file')\n",
    "    commit.put_file('./local_folder/some_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new commit in the master branch of repo 'example1',\n",
    "# upload multiple CSV files while keeping one level of the directory structure.\n",
    "# Finish the commit and block until all jobs that may have been triggered by\n",
    "# this commit have completed (flush).\n",
    "with client.commit('example1', flush=True) as commit:\n",
    "    commit.put_files('./local_folder/*/*.csv', keep_structure=True, base_path='./local_folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
